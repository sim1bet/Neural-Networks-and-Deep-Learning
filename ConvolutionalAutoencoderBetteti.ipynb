{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionalAutoencoderBetteti.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChPJw-tDfHY4"
      },
      "source": [
        "# UNSUPERVISED LEARNING TASK\n",
        "In this notebook I will implement a convolutional autoencoder able to learn and reproduce the distrubution of instances of the dataset, as well explore more advanced techniques to achieve similar goals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syJv9UE3LACf"
      },
      "source": [
        "!pip install --quiet optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFLLAtrTLVNl"
      },
      "source": [
        "import optuna\n",
        "\n",
        "optuna.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibDZJHwrfAP1"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math as mt\n",
        "import random\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrAU-69Iib8m"
      },
      "source": [
        "## Datasets\n",
        "Creation of the datasets: MNIST, and CIFAR (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3E-6gZfibcE"
      },
      "source": [
        "### Download the data and create dataset\n",
        "DataDir_MNIST = 'MNIST'\n",
        "DataDir_CIFAR = 'CIFAR10'\n",
        "\n",
        "train_dataset_MN = torchvision.datasets.MNIST(DataDir_MNIST, train=True, download=True)\n",
        "test_dataset_MN  = torchvision.datasets.MNIST(DataDir_MNIST, train=False, download=True)\n",
        "\n",
        "#train_dataset_CF = torchvision.datasets.CIFAR10(DataDir_CF, train=True, download=True)\n",
        "#test_dataset_CF  = torchvision.datasets.CIFAR10(DataDir_CF, train=False, download=True)\n",
        "\n",
        "dt=\"MN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI8B27LgmJeB"
      },
      "source": [
        "Definition of the transformation to operate on the training and test dataset (in the case of the CIFAR10 dataset, normalizing and cropping transformation will be applied only to the training set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJGivTormXMk"
      },
      "source": [
        "training_transf = transforms.Compose([transforms.ToTensor()])\n",
        "test_transf = transforms.Compose([transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wRCTMfJqlBP",
        "outputId": "cc7679d7-098b-4d39-c7ee-fc5c2a35b729"
      },
      "source": [
        "train_dataset_MN.transform = training_transf\n",
        "test_dataset_MN.transform = test_transf\n",
        "\n",
        "#train_dataset_CF.transform = training_transf\n",
        "#test_dataset_CF.transform = test_transf \n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX_fXfdYrdnc"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bRcWaYPriNV"
      },
      "source": [
        "### Encoder\n",
        "Definition of the convolutional encoder, exploring the application of both pooling and non linear layers to the filtered image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJIuYYzirgL6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoded_dim, proc, nh1, nh2, ch1, ch2, ch3, act_h, act_cnv, p):\n",
        "    super().__init__()\n",
        "\n",
        "    # Definition of the convolutional layer for the encoder\n",
        "    if proc=='pool':\n",
        "      self.enc_conv=nn.Sequential(\n",
        "          # 1st\n",
        "          nn.Conv2d(1, ch1, kernel_size=3, stride=2, padding=1),\n",
        "          nn.LPPool2d(2, kernel_size=2, stride=1),\n",
        "          # 2nd\n",
        "          nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
        "          nn.LPPool2d(2, kernel_size=2, stride=1),\n",
        "          # 3rd\n",
        "          nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=1)\n",
        "          # no pooling in the final layer\n",
        "      )\n",
        "    elif proc=='nlact':\n",
        "      if act_cnv=='SiL':\n",
        "        self.enc_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.Conv2d(1, ch1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.SiLU(True),\n",
        "            # 2nd\n",
        "            nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.SiLU(True),\n",
        "            # 3rd\n",
        "            nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=0)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      elif act_cnv=='Lk':\n",
        "        self.enc_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.Conv2d(1, ch1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(True),\n",
        "            # 2nd\n",
        "            nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(True),\n",
        "            # 3rd\n",
        "            nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=0)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      elif act_cnv=='Ge':\n",
        "        self.enc_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.Conv2d(1, ch1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            # 2nd\n",
        "            nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            # 3rd\n",
        "            nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=0)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      else:\n",
        "        print(\"Invalid input for the type of processing\")\n",
        "    \n",
        "    self.flatten=nn.Flatten(start_dim=1)\n",
        "\n",
        "    # Definition of the linear layer\n",
        "    if act_h==\"Lk\":\n",
        "      self.encoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear((ch3*3*3), nh1),\n",
        "        nn.LeakyReLU(True),\n",
        "        nn.Dropout(p),\n",
        "        #Second linear\n",
        "        nn.Linear(nh1, nh2),\n",
        "        nn.LeakyReLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Output Linear Layer\n",
        "        nn.Linear(nh2, encoded_dim)  \n",
        "        )  \n",
        "    elif act_h==\"Ge\":\n",
        "      self.encoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear((ch3*3*3), nh1),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh1, nh2),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p),\n",
        "        # Output Linear layer\n",
        "        nn.Linear(nh2, encoded_dim)  \n",
        "        )  \n",
        "    elif act_h==\"SiL\":\n",
        "      self.encoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear((ch3*3*3), nh1),\n",
        "        nn.SiLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh1, nh2),\n",
        "        nn.SiLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear layer\n",
        "        nn.Linear(nh2, encoded_dim)  \n",
        "        )  \n",
        "    elif act_h==\"Se\":\n",
        "      self.encoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear((ch3*3*3), nh1),\n",
        "        nn.SELU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh1, nh2),\n",
        "        nn.SELU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear Layer\n",
        "        nn.Linear(nh2, encoded_dim)  \n",
        "        )  \n",
        "    elif act_h==\"Si\":\n",
        "      self.encoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear((ch3*3*3), nh1),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh1, nh2),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear Layer\n",
        "        nn.Linear(nh2, encoded_dim)  \n",
        "        )  \n",
        "    \n",
        "  def forward(self,x):\n",
        "    #print(x.size())\n",
        "    x=self.enc_conv(x)\n",
        "    #print(x.size())\n",
        "    x=self.flatten(x)\n",
        "    #print(x.size())\n",
        "    out=self.encoder_lin(x)\n",
        "    #print(out.size())\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tobx5bM1JfE"
      },
      "source": [
        "### Decoder\n",
        "Definition of the decoder, with the re-application of the same layers in inversed order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9eSVWv3hXN"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoded_dim, proc, nh1, nh2, ch1, ch2, ch3, act_h, act_cnv, p):\n",
        "    super().__init__()\n",
        "\n",
        "    # Definition of the linear layer\n",
        "    if act_h==\"Lk\":\n",
        "      self.decoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear(encoded_dim, nh2),\n",
        "        nn.LeakyReLU(True),\n",
        "        nn.Dropout(p),\n",
        "        #Second linear\n",
        "        nn.Linear(nh2, nh1),\n",
        "        nn.LeakyReLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Output Linear Layer\n",
        "        nn.Linear(nh1, ch3*3*3)  \n",
        "        )  \n",
        "    elif act_h==\"Ge\":\n",
        "      self.decoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear(encoded_dim, nh2),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh2, nh1),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p),\n",
        "        # Output Linear layer\n",
        "        nn.Linear(nh1, ch3*3*3)  \n",
        "        )  \n",
        "    elif act_h==\"SiL\":\n",
        "      self.decoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear(encoded_dim, nh2),\n",
        "        nn.SiLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh2, nh1),\n",
        "        nn.SiLU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear layer\n",
        "        nn.Linear(nh1, ch3*3*3)  \n",
        "        )  \n",
        "    elif act_h==\"Se\":\n",
        "      self.decoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear(encoded_dim , nh2),\n",
        "        nn.SELU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh2, nh1),\n",
        "        nn.SELU(True),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear Layer\n",
        "        nn.Linear(nh1, ch3*3*3)  \n",
        "        )  \n",
        "    elif act_h==\"Si\":\n",
        "      self.decoder_lin=nn.Sequential(\n",
        "        # First Linear Layer\n",
        "        nn.Linear(encoded_dim, nh2),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(p),\n",
        "        # Second Linear Layer\n",
        "        nn.Linear(nh2, nh1),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(p),\n",
        "        # Third Linear Layer\n",
        "        nn.Linear(nh1, ch3*3*3)  \n",
        "        ) \n",
        "    \n",
        "    self.unflatten=nn.Unflatten(dim=1, unflattened_size=(ch3,3,3))\n",
        "\n",
        "    if proc=='pool':\n",
        "      self.dec_conv=nn.Sequential(\n",
        "          # 1st\n",
        "          nn.ConvTranspose2d(ch3, ch2, kernel_size=3, stride=2, padding=0, output_padding=0),\n",
        "          #nn.LPPool2d(2, kernel_size=3, stride=1),\n",
        "          # 2nd\n",
        "          nn.ConvTranspose2d(ch2, ch1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "          #nn.LPPool2d(2, kernel_size=2, stride=1),\n",
        "          # 3rd\n",
        "          nn.ConvTranspose2d(ch1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "          # no pooling in the final layer\n",
        "      )\n",
        "    elif proc=='nlact':\n",
        "      if act_cnv=='SiL':\n",
        "        self.dec_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.ConvTranspose2d(ch3, ch2, kernel_size=3, stride=2, output_padding=0),\n",
        "            nn.SiLU(True),\n",
        "            # 2nd\n",
        "            nn.ConvTranspose2d(ch2, ch1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.SiLU(True),\n",
        "            # 3rd\n",
        "            nn.ConvTranspose2d(ch1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      elif act_cnv=='Lk':\n",
        "        self.dec_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.ConvTranspose2d(ch3, ch2, kernel_size=3, stride=2, output_padding=0),\n",
        "            nn.LeakyReLU(True),\n",
        "            # 2nd\n",
        "            nn.ConvTranspose2d(ch2, ch1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(True),\n",
        "            # 3rd\n",
        "            nn.ConvTranspose2d(ch1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      elif act_cnv=='Ge':\n",
        "        self.dec_conv=nn.Sequential(\n",
        "            # 1st\n",
        "            nn.ConvTranspose2d(ch3, ch2, kernel_size=3, stride=2, output_padding=0),\n",
        "            nn.GELU(),\n",
        "            # 2nd\n",
        "            nn.ConvTranspose2d(ch2, ch1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.GELU(),\n",
        "            # 3rd\n",
        "            nn.ConvTranspose2d(ch1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "            # no non-linearity in the final layer\n",
        "        )\n",
        "      else:\n",
        "        print(\"Invalid input for the type of processing\")    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.decoder_lin(x)\n",
        "    #print(x.size())\n",
        "    x=self.unflatten(x)\n",
        "    #print(x.size())\n",
        "    x=self.dec_conv(x)\n",
        "    #print(x.size())\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSc6F8JJLkRw"
      },
      "source": [
        "### General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeNNOJBpLnTb"
      },
      "source": [
        "torch.manual_seed(2)\n",
        "\n",
        "nh1=128\n",
        "nh2=150\n",
        "\n",
        "ch1=8\n",
        "ch2=16\n",
        "ch3=32\n",
        "\n",
        "if dt==\"MN\":\n",
        "  train_dt=train_dataset_MN\n",
        "  test_dt=test_dataset_MN\n",
        "elif dt==\"CF\":\n",
        "  train_dt=train_dataset_CF\n",
        "  test_dt=test_dataset_CF\n",
        "img=test_dt[0][0].unsqueeze(0).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4xzHa6bNMWj"
      },
      "source": [
        "## Training\n",
        "Definition of the Encoder training function and the Decoder training function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbgM9bsng0GB"
      },
      "source": [
        "def RelEntr(mean, cov, encoded_dim):\n",
        "  # We are considering the relative entropy between two gaussians\n",
        "  RE = torch.sum((torch.sum(torch.exp(cov),1)+torch.sum(torch.square(mean),1)-encoded_dim-torch.log(torch.prod(torch.exp(cov),1)))/2)\n",
        "\n",
        "  return RE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6_W1LbBNTkB"
      },
      "source": [
        "def train_f(encoder, decoder, mean, covTrac, data, loss_fn, optimizer, device, auto_enc, encoded_dim, eph_loss):\n",
        "  # Definition of the training process on the basis of the two defined classes\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  if auto_enc==\"Variational\":\n",
        "    mean.train()\n",
        "    covTrac.train()\n",
        "  # The data is supposed to be already available in an iterable format\n",
        "  # Deliberate neglection of the labels\n",
        "  for images, _ in data:\n",
        "    images=images.to(device)\n",
        "    _=_.to(device)\n",
        "    # Processing\n",
        "    enc_data=encoder(images)\n",
        "    if auto_enc==\"Variational\":\n",
        "      mn=mean(enc_data)\n",
        "      cv=covTrac(enc_data)\n",
        "      q=torch.distributions.Normal(0,1)\n",
        "      xi=q.rsample()\n",
        "      enc_data=mn+xi*torch.sqrt(torch.exp(cv))\n",
        "    dec_data=decoder(enc_data)\n",
        "    # Loss (distance between the two datapoints according to a certain metric)\n",
        "    if auto_enc==\"Standard\":\n",
        "      Loss=loss_fn(dec_data,images)\n",
        "    elif auto_enc==\"Denoising\":\n",
        "      Loss=loss_fn(dec_data,_)\n",
        "    elif auto_enc==\"Variational\":\n",
        "      Loss=loss_fn(dec_data, images)+RelEntr(mn, cv, encoded_dim)\n",
        "      eph_loss.append(Loss.data.detach().cpu().numpy())\n",
        "    # Backprop\n",
        "    optimizer.zero_grad()\n",
        "    Loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('\\t partial train loss (single batch): %f' % (Loss.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhbmSCnxT5cx"
      },
      "source": [
        "def test_f(encoder, decoder, mean, covTrac, data, loss_fn, device, auto_enc):\n",
        "  # Definition of the evaluation process (both for validation and test set)\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "  with torch.no_grad():\n",
        "    # Definition of the lists where to store real and reconstructed images\n",
        "    Im_out=[]\n",
        "    Im_rec=[]\n",
        "    Im_lb=[]\n",
        "    for images, _ in data:\n",
        "      images=images.to(device)\n",
        "      # Processing\n",
        "      enc_data=encoder(images)\n",
        "      if auto_enc==\"Variational\":\n",
        "        mn=mean(enc_data)\n",
        "        cv=covTrac(enc_data)\n",
        "        enc_data=mn+cv\n",
        "      dec_data=decoder(enc_data)\n",
        "      # creation of lists\n",
        "      Im_out.append(dec_data.cpu())\n",
        "      Im_rec.append(images.cpu())\n",
        "      Im_lb.append(_.cpu())\n",
        "    # Conversion to tensor of the entire data structure\n",
        "    Im_out=torch.cat(Im_out)\n",
        "    Im_rec=torch.cat(Im_rec)\n",
        "    Im_lb=torch.cat(Im_lb)\n",
        "    # Computation of the final loss\n",
        "    Loss=loss_fn(Im_rec, Im_out)\n",
        "  return Im_out, Im_lb, Loss.data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyHIbFfiJpPD"
      },
      "source": [
        "### Standard Autoencoder\n",
        "Definition of the main loop for the training of the encoder and the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wRDh0z5khot"
      },
      "source": [
        "Use of the Optuna library to perform random search on the space of hyperparameters, where the chosen hp are:\n",
        "\n",
        "\n",
        "*   Type of convolutional processing: pooling ('pool'), non-linear ('nlact') \n",
        "*   Type of convolutional activation: SiLU ('SiL'), LeakyReLU ('Lk'), GELU ('Ge')\n",
        "* Type of activation (linear layers): SiLU ('SiL'), LeakyReLU ('Lk'), GELU ('Ge'), SELU ('Se'), Sigmoid ('Si')\n",
        "* Learning rate\n",
        "* Weight decay rate\n",
        "\n",
        "The number of units (as well as the number of channels) is kept fixed in order not to increase drastically the size of the hyperparameters space to search.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J77zCq15L1O_"
      },
      "source": [
        "def objective(trial):\n",
        "\n",
        "  global tr\n",
        "  global kfld\n",
        "  global n_trials, auto_enc\n",
        "\n",
        "  act_h = trial.suggest_categorical('act_h',['Lk','Ge','SiL','Se','Si'])\n",
        "  act_cnv = trial.suggest_categorical('act_cnv',['SiL','Lk','Ge'])\n",
        "  if auto_enc=='Variational':\n",
        "    act_v = trial.suggest_categorical('act_v',['Lk','Re','Se'])\n",
        "\n",
        "  proc = trial.suggest_categorical('proc',['pool','nlact','nlact','nlact'])\n",
        "  encoded_dim = trial.suggest_int('encoded_dim',10,30)\n",
        "  p = trial.suggest_uniform('p',0.01,0.1)\n",
        "\n",
        "  encoder=Encoder(encoded_dim, proc, nh1, nh2, ch1, ch2, ch3, act_h=act_h, act_cnv=act_cnv, p=p)\n",
        "\n",
        "  if auto_enc=='Variational':\n",
        "    mean=StatNet(encoded_dim, act_v)\n",
        "    covTrac=StatNet(encoded_dim, act_v)\n",
        "\n",
        "    mean.to(device)\n",
        "    covTrac.to(device)\n",
        "  else:\n",
        "    mean='boh'\n",
        "    covTrac='boh'\n",
        "\n",
        "  decoder=Decoder(encoded_dim, proc, nh1, nh2, ch1, ch2, ch3, act_h=act_h, act_cnv=act_cnv, p=p)\n",
        "  encoder.to(device)\n",
        "  decoder.to(device)\n",
        "\n",
        "  lr = trial.suggest_uniform('lr',1e-4,1e-2) #4-2\n",
        "  weight_dc = trial.suggest_uniform('weight_dc',1e-6,1e-5) #6-5\n",
        "  rec_loss_vl=[]\n",
        "  rec_loss_tr=[]\n",
        "\n",
        "  if auto_enc=='Variational':  \n",
        "    parameters_to_opt=[{'params': encoder.parameters()},\n",
        "                       {'params': mean.parameters()},\n",
        "                       {'params': covTrac.parameters()},\n",
        "                       {'params': decoder.parameters()}\n",
        "                      ]\n",
        "    Model={'proc':proc,\n",
        "         'act_h':act_h, \n",
        "         'act_cnv':act_cnv, \n",
        "         'act_v':act_v,\n",
        "         'lr':lr, \n",
        "         'weight_dc':weight_dc, \n",
        "         'trial':tr, \n",
        "         'kfold':kfld,\n",
        "         'encoded_dim':encoded_dim,\n",
        "         'p':p}\n",
        "  else:\n",
        "    parameters_to_opt=[{'params': encoder.parameters()},\n",
        "                      {'params': decoder.parameters()}\n",
        "                      ]\n",
        "    Model={'proc':proc,\n",
        "         'act_h':act_h, \n",
        "         'act_cnv':act_cnv, \n",
        "         'lr':lr, \n",
        "         'weight_dc':weight_dc, \n",
        "         'trial':tr, \n",
        "         'kfold':kfld,\n",
        "         'encoded_dim':encoded_dim,\n",
        "         'p':p}\n",
        "\n",
        "  optimizer=optim.Adam(parameters_to_opt, lr=lr, weight_decay=weight_dc)\n",
        "  Models.append(Model)\n",
        "\n",
        "  for eph in range(n_eph):\n",
        "    # (use of the training function)\n",
        "    eph_loss=[]\n",
        "    train_f(encoder=encoder,\n",
        "            mean=mean,\n",
        "            covTrac=covTrac,\n",
        "            decoder=decoder,\n",
        "            data=train_dts,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "            auto_enc=auto_enc,\n",
        "            encoded_dim=encoded_dim,\n",
        "            eph_loss=eph_loss) \n",
        "    _,_,val_loss=test_f(encoder=encoder,\n",
        "                        decoder=decoder,\n",
        "                        mean=mean,\n",
        "                        covTrac=covTrac,\n",
        "                        data=val_dts,\n",
        "                        loss_fn=loss_fn,\n",
        "                        device=device,\n",
        "                        auto_enc=auto_enc)\n",
        "    rec_loss_vl.append(val_loss.cpu().numpy())\n",
        "    if auto_enc=='Variational':\n",
        "      rec_loss_tr.append(eph_loss)\n",
        "\n",
        "    if eph%5==0:\n",
        "      print(\"-----------------------\")\n",
        "      print(\"Epoch: \"+str(eph))\n",
        "      print(\"Validation Loss: \"+str(val_loss.cpu().numpy()))\n",
        "      print(\"-----------------------\")\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    if auto_enc=='Variational':\n",
        "      Final_Act=nn.Sigmoid()\n",
        "      with torch.no_grad():\n",
        "        rec_img=Final_Act(decoder(mean(encoder(img))+covTrac(encoder(img))))\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        rec_img=decoder(encoder(img))\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
        "    axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "    axs[0].set_title('Original Image')\n",
        "    axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "    axs[1].set_title('Reconstructed Image (epoch - %d)' %(eph))\n",
        "    plt.tight_layout()\n",
        "    if eph==(n_eph-1):\n",
        "      name=\"Rec_img_\"+auto_enc+\"_\"+str(tr)+'_'+str(kfld)+'.png'\n",
        "      plt.savefig(name)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "  Rec_L={'Rec_Loss_vl':rec_loss_vl, 'Rec_Loss_tr':rec_loss_tr, 'trial':tr, 'kfold':kfld}\n",
        "  Rec_Losses.append(Rec_L)\n",
        "  \n",
        "  plt.figure(figsize=(12,8))\n",
        "  if auto_enc=='Variational':\n",
        "    plt.semilogy([np.mean(x) for x in rec_loss_tr])\n",
        "  else:\n",
        "    plt.semilogy(rec_loss_vl)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.grid()\n",
        "  name_loss=\"Rec_Loss_\"+auto_enc+\"_\"+str(tr)+'_'+str(kfld)+'.png'\n",
        "  plt.savefig(name_loss)\n",
        "  plt.show()  \n",
        "\n",
        "  # Save the parameters of the encoder and the decoder\n",
        "  nameEnc='EncoderParameters_'+auto_enc+'_'+str(tr)+'_'+str(kfld)+'.pth'\n",
        "  nameDec='DecoderParameters_'+auto_enc+'_'+str(tr)+'_'+str(kfld)+'.pth'\n",
        "  torch.save(encoder.state_dict(),nameEnc)\n",
        "  torch.save(decoder.state_dict(),nameDec)\n",
        "\n",
        "  if auto_enc=='Variational':\n",
        "    nameMean='Mean_'+str(tr)+'_'+str(kfld)+'.pth'\n",
        "    nameCov='CovTrace_'+str(tr)+'_'+str(kfld)+'.pth'\n",
        "    torch.save(mean.state_dict(), nameMean)\n",
        "    torch.save(covTrac.state_dict(), nameCov)\n",
        "\n",
        "  tr+=1\n",
        "\n",
        "  return val_loss.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ndfNsCogPa"
      },
      "source": [
        "Functions for the definition of the five best models, based on statistics of the validation loss for each model (mean)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReKAT_Qime5n"
      },
      "source": [
        "def BestModels():\n",
        "  global auto_enc\n",
        "\n",
        "  means=[]\n",
        "  for rec in Rec_Losses:\n",
        "    #if auto_enc=='Variational':\n",
        "      #if rec['Rec_Loss_tr'][-1][-1]=='nan':\n",
        "      #  mean=1e+15\n",
        "      #else:\n",
        "        #mean=np.mean([np.mean(x) for x in rec['Rec_Loss_tr'][-5]])\n",
        "    #else:\n",
        "    if rec['Rec_Loss_vl'][-1]=='nan':\n",
        "      mean=1e+15\n",
        "    else:\n",
        "      mean=np.mean(rec['Rec_Loss_vl'][-5:])\n",
        "    means.append(mean)\n",
        "  top=np.copy(means)\n",
        "  top.sort()\n",
        "  Best_Rec_idx=[]\n",
        "  for rec in top[:5]:\n",
        "    Best_Rec_idx.append(means.index(rec))\n",
        "  \n",
        "  return Best_Rec_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyXrlfRgJrbB"
      },
      "source": [
        "n_eph=10\n",
        "kf=KFold(n_splits=5)\n",
        "n_trials=15\n",
        "\n",
        "kfld=1\n",
        "Models=[]\n",
        "Rec_Losses=[]\n",
        "auto_enc=\"Standard\"\n",
        "\n",
        "loss_fn=nn.SmoothL1Loss()\n",
        "\n",
        "for train_s, val in kf.split(train_dt):\n",
        "\n",
        "  # Creation of the training and validation sets (folded)\n",
        "  train_dts=DataLoader([(train_dataset_MN[x][0],train_dataset_MN[x][1]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  val_dts=DataLoader([(train_dataset_MN[x][0],train_dataset_MN[x][1]) for x in val], batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "  tr=0\n",
        "\n",
        "  # Creation of the study\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  \n",
        "  kfld+=1\n",
        "\n",
        "Rec_Losses_Standard=np.copy(Rec_Losses)\n",
        "Models_Standard=np.copy(Models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvGFYECks9NG"
      },
      "source": [
        "Definition of the best models and evaluation of their reconstruction error on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYhu6znctRpY"
      },
      "source": [
        "test_dts=DataLoader([(test_dt[x][0],test_dt[x][1]) for x in range(len(test_dt))],batch_size=256, shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH13YWVUuxv_"
      },
      "source": [
        "def TestErrors(idx, hook=False):\n",
        "\n",
        "  global rank, auto_enc\n",
        "  global nh1, nh2, ch1, ch2, ch3\n",
        "\n",
        "  model=Models[idx]\n",
        "  \n",
        "  encoder=Encoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "  decoder=Decoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "  if auto_enc=='Variational':\n",
        "    mean=StatNet(model['encoded_dim'], model['act_v'])\n",
        "    covTrac=StatNet(model['encoded_dim'], model['act_v'])\n",
        "\n",
        "    nameMean='Mean_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "    nameCov='CovTrace_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "    mean.load_state_dict(torch.load(nameMean))\n",
        "    covTrac.load_state_dict(torch.load(nameCov))\n",
        "    mean.to(device)\n",
        "    covTrac.to(device)\n",
        "  else:\n",
        "    mean='boh'\n",
        "    covTrac='boh'\n",
        "\n",
        "  name_enc_par='EncoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "  name_dec_par='DecoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "  encoder.load_state_dict(torch.load(name_enc_par))\n",
        "  decoder.load_state_dict(torch.load(name_dec_par))\n",
        "  encoder.to(device)\n",
        "  decoder.to(device)\n",
        "  \n",
        "  if hook==True:\n",
        "    hook_handle=encoder.encoder_lin[6].register_forward_hook(hook_feature_space)\n",
        "\n",
        "  Im_out , Im_lb ,test_error=test_f(encoder=encoder,\n",
        "                        decoder=decoder,\n",
        "                        mean=mean,\n",
        "                        covTrac=covTrac,\n",
        "                        data=test_dts,\n",
        "                        loss_fn=loss_fn,\n",
        "                        device=device,\n",
        "                        auto_enc=auto_enc\n",
        "                        )\n",
        "  \n",
        "  if hook==False:\n",
        "    print()\n",
        "    print('Validation Rank: '+str(rank))\n",
        "    print('------------------------------')\n",
        "    print('Test Reconstruction Error: '+str(test_error.cpu().numpy()))\n",
        "    print('------------------------------')\n",
        "    print(model)\n",
        "    print('------------------------------')\n",
        "  \n",
        "  rank+=1\n",
        "\n",
        "  return {'Test_Rec_err':test_error.data,'Model':model}, Im_out, Im_lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TddWI2S-ybmV"
      },
      "source": [
        "Best_Models_Standard=BestModels()\n",
        "\n",
        "test_err=[]\n",
        "rank=1\n",
        "for idx in Best_Models_Standard:\n",
        "  tst_err,_,_=TestErrors(idx)\n",
        "  test_err.append(tst_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uKWeK8PRj44"
      },
      "source": [
        "### Denoising Autoencoder\n",
        "In this section of the notebook I add noise to the input in order to force the NN to learn the data distribution instead of simply reproducing a pseudo-identity function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qmzFf_kWhYH"
      },
      "source": [
        "Creation of a function for the introduction of different types of noise in the dataset (Gaussian, Uniform, Exponential, Gamma)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H02wFtlR_0D"
      },
      "source": [
        "class AddNoise(object):\n",
        "\n",
        "  def __init__(self, noise):\n",
        "    self.noise=noise\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    if self.noise==\"Norm\":\n",
        "      return tensor + torch.randn(tensor.size())*0.1\n",
        "    elif self.noise==\"Uni\":\n",
        "      return tensor + torch.Tensor(np.random.uniform(-1,1,size=tensor.size()))*0.1\n",
        "    elif self.noise==\"Exp\":\n",
        "      return tensor + torch.Tensor(np.multiply(np.random.randint(-1,1,tensor.size()),np.random.exponential(scale=2.5, size=tensor.size())))*0.1\n",
        "    elif self.noise==\"Gamma\":\n",
        "      return tensor + torch.Tensor(np.multiply(np.random.randint(-1,1,tensor.size()),np.random.gamma(shape=5,scale=3, size=tensor.size())))*0.1\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__+\"(Noise={})\".format(self.noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dVg_9wDg3t1"
      },
      "source": [
        "transform_norm = transforms.Compose([AddNoise(\"Norm\")])\n",
        "transform_uni = transforms.Compose([AddNoise(\"Uni\")])\n",
        "transform_exp = transforms.Compose([AddNoise(\"Exp\")])\n",
        "transform_gam = transforms.Compose([AddNoise(\"Gamma\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj6RUZ_nonEu"
      },
      "source": [
        "Definition of the training loop for the denoising autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq_bFQSdorxq"
      },
      "source": [
        "n_eph=10\n",
        "kf=KFold(n_splits=4)\n",
        "n_trials=15\n",
        "\n",
        "kfld=1\n",
        "Models=[]\n",
        "Rec_Losses=[]\n",
        "auto_enc=\"Denoising\"\n",
        "\n",
        "loss_fn=nn.SmoothL1Loss()\n",
        "\n",
        "for train_s, val in kf.split(train_dt):\n",
        "\n",
        "  # Creation of the training and validation sets (folded)\n",
        "  if kfld==1:\n",
        "    train_dts=DataLoader([(transform_norm(train_dt[x][0]),train_dt[x][0]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  elif kfld==2:\n",
        "    train_dts=DataLoader([(transform_uni(train_dt[x][0]),train_dt[x][0]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  elif kfld==3:\n",
        "    train_dts=DataLoader([(transform_exp(train_dt[x][0]),train_dt[x][0]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  elif kfld==4:\n",
        "    train_dts=DataLoader([(transform_gam(train_dt[x][0]),train_dt[x][0]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  val_dts=DataLoader([(train_dt[x][0],train_dt[x][1]) for x in val], batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "  tr=0\n",
        "\n",
        "  # Creation of the study\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  \n",
        "  kfld+=1\n",
        "\n",
        "Rec_Losses_Denoising=np.copy(Rec_Losses)\n",
        "Models_Denoising=np.copy(Models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjh1lYuPxMcP"
      },
      "source": [
        "Best_Models_Denoising=BestModels()\n",
        "\n",
        "test_err=[]\n",
        "rank=1\n",
        "for idx in Best_Models_Denoising:\n",
        "  tst_err,_,_=TestErrors(idx)\n",
        "  test_err.append(tst_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCqKKox5lxn"
      },
      "source": [
        "### Supervised Fine-Tuning\n",
        "Application of transfer learning on the correct classification of images in the MNIST dataset - performed only for the two best performing standard autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW2zuDOiSj_M"
      },
      "source": [
        "In the first approach, I consider only the encoder, freeze its parameters, and stack a simple classification model on top of it, in order to evaluate if, based on the extracted latent code, the whole model is faster and more accurate than the one built for the first homework. (I will directly use the activation function and hyperparameters that performed better)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrTTUVlVTBQF"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "\n",
        "  def __init__(self, Ni, n_or, nh1_cl, nh2_cl, No):\n",
        "    super().__init__()\n",
        "    # Activation function\n",
        "    self.act=nn.SiLU()\n",
        "    # NN layers\n",
        "    self.ori=nn.Linear(Ni, n_or)\n",
        "    self.fc1=nn.Linear(n_or, nh1_cl)\n",
        "    self.fc2=nn.Linear(nh1_cl, nh2_cl)\n",
        "    self.out=nn.Linear(nh2_cl, No)\n",
        "\n",
        "  def forward(self, sample):\n",
        "    x=self.ori(sample)\n",
        "    x=self.act(self.fc1(x))\n",
        "    x=self.act(self.fc2(x))\n",
        "    out=self.out(x)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "343W2j6TWOLo"
      },
      "source": [
        "def train_clas(encoder, classifier, data, device, loss_fn_class, optimizer_class):\n",
        "  # Setting to train for both the encoder and the classifier to perform gradient descent (or analogous optimization procedure)\n",
        "  encoder.train()\n",
        "  classifier.train()\n",
        "  for image, label in data:\n",
        "    image=image.to(device)\n",
        "    label=label.to(device)\n",
        "    # Processing of the image (batch)\n",
        "    enc_im=encoder(image)\n",
        "    emp_label=classifier(enc_im)\n",
        "    # Loss\n",
        "    Loss=loss_fn_class(emp_label, label)\n",
        "    optimizer_class.zero_grad()\n",
        "    Loss.backward()\n",
        "    optimizer_class.step()\n",
        "    # Loss for the single batch\n",
        "    #print('\\t Training loss (single batch):', float(loss.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9WcnvXJZGcG"
      },
      "source": [
        "def test_clas(encoder, classifier, data, device, loss_fn_class):\n",
        "  # Setting in evaluation mode both the encoder and the classifier\n",
        "  encoder.eval()\n",
        "  classifier.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    Im_emp_lb=[]\n",
        "    Im_lb=[]\n",
        "    for image, label in data:\n",
        "      image=image.to(device)\n",
        "      label=label.to(device)\n",
        "      # Processing of the image\n",
        "      enc_im=encoder(image)\n",
        "      emp_label=classifier(enc_im)\n",
        "      # Storing of new and old information\n",
        "      Im_emp_lb.append(emp_label.cpu())\n",
        "      Im_lb.append(label.cpu())\n",
        "    Im_emp_lb=torch.cat(Im_emp_lb)\n",
        "    Im_lb=torch.cat(Im_lb)\n",
        "    Loss=loss_fn_class(Im_emp_lb,Im_lb)\n",
        "\n",
        "  return Loss.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZsNnkuhcjA3"
      },
      "source": [
        "auto_enc='Standard'\n",
        "kf=KFold(n_splits=2)\n",
        "\n",
        "n_or=784\n",
        "nh1_cl=128\n",
        "nh2_cl=128\n",
        "No=10\n",
        "\n",
        "n_eph=35\n",
        "loss_fn_class=nn.CrossEntropyLoss()\n",
        "Loss=[]\n",
        "\n",
        "i=0\n",
        "for train_idx, val_idx in kf.split(train_dt):\n",
        "  class_train_dt=DataLoader([(train_dt[x][0],train_dt[x][1]) for x in train_idx], batch_size=256, shuffle=True, num_workers=0)\n",
        "  class_val_dt=DataLoader([(train_dt[x][0],train_dt[x][1]) for x in val_idx], batch_size=256, shuffle=False, num_workers=0)\n",
        "  for idx in Best_Models_Standard[:3]:\n",
        "    model=Models_Standard[idx]\n",
        "    encoder=Encoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "    name_enc_par='EncoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "    encoder.load_state_dict(torch.load(name_enc_par))\n",
        "    for param_name, param in encoder.named_parameters():\n",
        "      param.requires_grad=False\n",
        "    classifier=Classifier(model['encoded_dim'], n_or, nh1_cl, nh2_cl, No)\n",
        "    encoder.to(device)\n",
        "    classifier.to(device)\n",
        "\n",
        "    params_class=[{\"params\":encoder.parameters()},\n",
        "                  {\"params\":classifier.parameters()}]\n",
        "    optimizer_class=optim.Adam(params_class, lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "    # Training\n",
        "    loss_class=[]\n",
        "    for eph in range(n_eph):\n",
        "      train_clas(encoder=encoder,\n",
        "                 classifier=classifier,\n",
        "                 data=class_train_dt,\n",
        "                 device=device,\n",
        "                 loss_fn_class=loss_fn_class,\n",
        "                 optimizer_class=optimizer_class\n",
        "                )\n",
        "      val_loss=test_clas(encoder=encoder,\n",
        "                         classifier=classifier,\n",
        "                         data=class_val_dt,\n",
        "                         device=device,\n",
        "                         loss_fn_class=loss_fn_class)\n",
        "      \n",
        "      loss_class.append(val_loss.cpu().numpy())\n",
        "\n",
        "      if eph%5==0:\n",
        "        print(\"-----------------------\")\n",
        "        print(\"Epoch: \"+str(eph))\n",
        "        print(\"Validation Loss: \"+str(val_loss.cpu().numpy()))\n",
        "        print(\"-----------------------\")\n",
        "\n",
        "    Loss.append({\"model\":model,\"val_loss\":loss_class})\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.semilogy(loss_class)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    name_class=\"Class_\"+str(model[\"trial\"])+'_'+str(model[\"kfold\"])+'_'+str(i)+'.png'\n",
        "    plt.savefig(name_class)\n",
        "    plt.show() \n",
        "\n",
        "    name_classifier='Classifier_'+str(idx)+'_'+str(i)+'.pth'\n",
        "    torch.save(classifier.state_dict(), name_classifier)\n",
        "\n",
        "  i+=1\n",
        "\n",
        "for idx in Best_Models_Standard[:3]:\n",
        "  for i in range(2):\n",
        "    model = Models_Standard[idx]\n",
        "    encoder=Encoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "    classifier=Classifier(model['encoded_dim'], n_or, nh1_cl, nh2_cl, No)\n",
        "\n",
        "    name_enc_par='EncoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "    encoder.load_state_dict(torch.load(name_enc_par))\n",
        "    name_clas_par='Classifier_'+str(idx)+'_'+str(i)+'.pth'\n",
        "    classifier.load_state_dict(torch.load(name_clas_par))\n",
        "    encoder.to(device)\n",
        "    classifier.to(device)\n",
        "\n",
        "    test_loss=test_clas(encoder=encoder,\n",
        "                        classifier=classifier,\n",
        "                        data=test_dts,\n",
        "                        device=device,\n",
        "                        loss_fn_class=loss_fn_class)\n",
        "    \n",
        "    print('----------------------------')\n",
        "    print('Fold: '+str(i))\n",
        "    print('----------------------------')\n",
        "    print('Test error: '+str(test_loss.cpu().numpy()))\n",
        "    print('----------------------------')\n",
        "    print(model)\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBFtGvInhcKG"
      },
      "source": [
        "### Latent Space Analysis\n",
        "The analysis exploits the latent code of the best denoising model (hence the one which should have been better at capturing the original data dsitribution) and applies PCA and t-SNE to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1xD744f5rMT"
      },
      "source": [
        "def hook_feature_space(module, input, output):\n",
        "  latent_space.append(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTJ3jPQo6VN9"
      },
      "source": [
        "latent_space=[]\n",
        "id = Best_Models_Denoising[0]\n",
        "auto_enc='Denoising'\n",
        "\n",
        "_, Im_out, Im_lb = TestErrors(id, hook=True)\n",
        "\n",
        "latent_space=torch.cat(latent_space)\n",
        "print(latent_space)\n",
        "\n",
        "latent_space=latent_space.cpu()\n",
        "Im_out=Im_out.cpu()\n",
        "Im_lb=Im_lb.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCykvE1kAL5"
      },
      "source": [
        "pca=PCA(n_components=2)\n",
        "latent_space_pca=pca.fit_transform(latent_space)\n",
        "\n",
        "df = pd.DataFrame(latent_space_pca)\n",
        "df['label'] = Im_lb\n",
        "\n",
        "px.scatter(df, x=0, y=1, color=df.label.astype(str), hover_data=[df.index], title='PCA - latent space')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtaRcbnShhnd"
      },
      "source": [
        "tsne = TSNE(n_components=2, metric='euclidean', init='pca', random_state=0)\n",
        "name = tsne.fit_transform(latent_space)\n",
        "\n",
        "df_t = pd.DataFrame(name)\n",
        "df_t['label'] = Im_lb\n",
        "\n",
        "title='tSNE Euclidean - latent space'\n",
        "px.scatter(df_t, x=0, y=1, color=df_t.label.astype(str), hover_data=[df_t.index], title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejEMnwSgh3WQ"
      },
      "source": [
        "tsne = TSNE(n_components=2, metric='cosine', init='pca', random_state=0)\n",
        "name = tsne.fit_transform(latent_space)\n",
        "\n",
        "df_t = pd.DataFrame(name)\n",
        "df_t['label'] = Im_lb\n",
        "\n",
        "title='tSNE Cosine - latent space'\n",
        "px.scatter(df_t, x=0, y=1, color=df_t.label.astype(str), hover_data=[df_t.index], title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQAKagoAkHDT"
      },
      "source": [
        "I now try to feed to the decoder a vector of the dimension of the encoded space presenting value randomly drawn from specific probability distributions, and see which kinds of images i am able to retrieve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9MU2LqvlVaB"
      },
      "source": [
        "def image_gen(decoder, encoded_d, mean, covTrac, auto_enc):\n",
        "  distributions=['beta','exponential','gamma','normal','uniform']\n",
        "  for distribution in distributions:\n",
        "    if distribution=='beta':\n",
        "      random_enc=torch.Tensor([(np.random.randint(-1,1))*(x**(0))*np.random.beta(5,1) for x in range(encoded_d)])\n",
        "    elif distribution=='exponential':\n",
        "      random_enc=torch.Tensor([(np.random.randint(-1,1))*(x**(0))*np.random.exponential(scale=2.5) for x in range(encoded_d)])  # value found in many exp law\n",
        "    elif distribution=='gamma':\n",
        "      random_enc=torch.Tensor([(np.random.randint(-1,1))*(x**(0))*np.random.gamma(shape=5, scale=3) for x in range(encoded_d)])\n",
        "    elif distribution=='normal':\n",
        "      random_enc=torch.Tensor([(x^(0))*np.random.normal() for x in range(encoded_d)])\n",
        "    elif distribution=='uniform':\n",
        "      random_enc=torch.Tensor([(x^(0))*np.random.uniform(-1,1) for x in range(encoded_d)])\n",
        "\n",
        "    random_enc=random_enc.unsqueeze(0).to(device)\n",
        "    print(random_enc)\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      if auto_enc=='Variational':\n",
        "        Final_Act=nn.Sigmoid()\n",
        "        #rec_img_rand=Final_Act(decoder(random_enc))\n",
        "        rec_img_rand=Final_Act(decoder(mean(random_enc)+covTrac(random_enc)))\n",
        "      else:\n",
        "        rec_img_rand=decoder(random_enc)\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.imshow(rec_img_rand.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "    plt.title('Reconstructed Image (distribution - %s)' %(distribution))  \n",
        "    name=\"Rec_img_\"+distribution+\"_\"+'.png'\n",
        "    plt.savefig(name)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rllEwodgO2wT"
      },
      "source": [
        "auto_enc='Denoising'\n",
        "id=Best_Models_Denoising[0]\n",
        "\n",
        "model=Models_Denoising[id]\n",
        "mean_d='boh'\n",
        "covTrac_d='boh'\n",
        "encoded_d=model['encoded_dim']\n",
        "decoder_d=Decoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "name_dec_par='DecoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "decoder_d.load_state_dict(torch.load(name_dec_par))\n",
        "decoder_d.to(device)\n",
        "\n",
        "image_gen(decoder_d, encoded_d, mean_d, covTrac_d, auto_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG_vQnDLtfZZ"
      },
      "source": [
        "### Variational Autoencoder\n",
        "Finally, I try to implement a variational autoencoder, which should improve the overall performance w.r.t. the one of the standard or denoising autoencoder. However, such improvement might be hard to appreciate due to the 'statistical simplicity' inherent in the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHh5xiZ3l1I"
      },
      "source": [
        "class StatNet(nn.Module):\n",
        "  def __init__(self, ni, act_v):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ni=ni\n",
        "    self.act_v=act_v\n",
        "\n",
        "    if act_v=='Lk':\n",
        "      self.mn=nn.Sequential(\n",
        "                          nn.LeakyReLU(),\n",
        "                          nn.Linear(ni, ni))\n",
        "    elif act_v=='Re':\n",
        "      self.mn=nn.Sequential(\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(ni, ni))\n",
        "    elif act_v=='Se':\n",
        "      self.mn=nn.Sequential(\n",
        "                          nn.SELU(),\n",
        "                          nn.Linear(ni, ni))\n",
        "\n",
        "  def forward(self, sample):\n",
        "    stat=self.mn(sample)\n",
        "    return stat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HWguXbfThdm"
      },
      "source": [
        "n_eph=10\n",
        "kf=KFold(n_splits=5)\n",
        "n_trials=15\n",
        "\n",
        "kfld=1\n",
        "Models=[]\n",
        "Rec_Losses=[]\n",
        "auto_enc='Variational'\n",
        "\n",
        "loss_fn=nn.BCEWithLogitsLoss(reduction='sum')  #BCE reduction=sum\n",
        "\n",
        "for train_s, val in kf.split(train_dt):\n",
        "\n",
        "  # Creation of the training and validation sets (folded)\n",
        "  train_dts=DataLoader([(train_dataset_MN[x][0],train_dataset_MN[x][1]) for x in train_s], batch_size=256, shuffle=True, num_workers=0)\n",
        "  n_batches=len(train_dts)\n",
        "  val_dts=DataLoader([(train_dataset_MN[x][0],train_dataset_MN[x][1]) for x in val], batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "  tr=0\n",
        "\n",
        "  # Creation of the study\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  \n",
        "  kfld+=1\n",
        "\n",
        "Rec_Losses_Variational=np.copy(Rec_Losses)\n",
        "Models_Variational=np.copy(Models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Ap0RWGN0MJ"
      },
      "source": [
        "Best_Models_Variational=BestModels()\n",
        "print(Best_Models_Variational)\n",
        "\n",
        "test_err=[]\n",
        "rank=1\n",
        "for idx in Best_Models_Variational:\n",
        "  tst_err,_,_=TestErrors(idx)\n",
        "  test_err.append(tst_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-f_r0aTt492"
      },
      "source": [
        "def ImagesVar(decoder, mean, cov, encoded_dim):\n",
        "  norm_distr=torch.randn(size=(25,encoded_dim), requires_grad=False).to(device)\n",
        "  Final_Act=nn.Sigmoid()\n",
        "  decoder.eval()\n",
        "  mean.eval()\n",
        "  cov.eval()\n",
        "  rec_img=Final_Act(decoder(mean(norm_distr)+cov(norm_distr)))\n",
        "\n",
        "  fig,axs=plt.subplots(5,5, figsize=(8,8))\n",
        "  cnt=0\n",
        "  for i in range(5):\n",
        "    for j in range(5):\n",
        "      axs[i,j].imshow(rec_img[cnt,:].detach().cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "      axs[i,j].axis('off')\n",
        "      cnt+=1\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('Samples_Variational')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyt4AKYGPBJs"
      },
      "source": [
        "auto_enc='Variational'\n",
        "id=Best_Models_Variational[0]\n",
        "\n",
        "model=Models_Variational[id]\n",
        "encoded_v=model['encoded_dim']\n",
        "decoder_v=Decoder(model['encoded_dim'], model['proc'], nh1, nh2, ch1, ch2, ch3, act_h=model['act_h'], act_cnv=model['act_cnv'], p=model['p'])\n",
        "mean_v=StatNet(model['encoded_dim'], model['act_v'])\n",
        "covTrac_v=StatNet(model['encoded_dim'], model['act_v'])\n",
        "\n",
        "name_dec_par='DecoderParameters_'+auto_enc+'_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "nameMean='Mean_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "nameCov='CovTrace_'+str(model['trial'])+'_'+str(model['kfold'])+'.pth'\n",
        "\n",
        "decoder_v.load_state_dict(torch.load(name_dec_par))\n",
        "mean_v.load_state_dict(torch.load(nameMean))\n",
        "covTrac_v.load_state_dict(torch.load(nameCov))\n",
        "\n",
        "decoder_v.to(device)\n",
        "mean_v.to(device)\n",
        "covTrac_v.to(device)\n",
        "\n",
        "\n",
        "image_gen(decoder_v, encoded_v, mean_v, covTrac_v, auto_enc)\n",
        "ImagesVar(decoder_v, mean_v, covTrac_v, encoded_v)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}