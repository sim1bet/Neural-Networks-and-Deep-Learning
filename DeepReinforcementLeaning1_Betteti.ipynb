{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepReinforcementLeaning1_Betteti.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25f4c86849684d58ad315672c964fe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08ba87bfe6cf4a4591a56d59bec7e290",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_20a4fae9f57640eba8e8a2993875da44",
              "IPY_MODEL_16b04bb0744749e3931df335f83abda7"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "08ba87bfe6cf4a4591a56d59bec7e290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "20a4fae9f57640eba8e8a2993875da44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1a0f8d1610a49508d899f901746acc5",
            "_dom_classes": [],
            "description": " 22%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 222,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0235bb00b4404097987ee9ff7cb49fd3"
          },
          "model_module_version": "1.5.0"
        },
        "16b04bb0744749e3931df335f83abda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59b1cbe0a57a4d0aab89aa18821597fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 222/1000 [00:31&lt;04:50,  2.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21bb2cd2d48c48d099cf42ff98c60170"
          },
          "model_module_version": "1.5.0"
        },
        "f1a0f8d1610a49508d899f901746acc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "0235bb00b4404097987ee9ff7cb49fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "59b1cbe0a57a4d0aab89aa18821597fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "21bb2cd2d48c48d099cf42ff98c60170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UwziVRe4H7-"
      },
      "source": [
        "# DEEP REINFORCEMENT LEARNING - CARTPOLE\n",
        "In this notebook I will explore the implementation of a deep network to solve a reinforcement learning task.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlWVtmxL4B_p",
        "outputId": "94c98415-3c49-41eb-d4cb-d1c295368b6b"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpuZlDT4cu5"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch import nn\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQZ870xZ4lqp",
        "outputId": "5e95e78f-0a7d-454f-878b-42d354693a1c"
      },
      "source": [
        "!apt update\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [552 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,387 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,724 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,352 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,921 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,158 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [882 kB]\n",
            "Fetched 11.2 MB in 3s (3,390 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (487 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 146456 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (699 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 148811 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZxv2RW4mTG"
      },
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wju-mZqH42NE"
      },
      "source": [
        "Creation of the display (virtual monitor) to visualize the OpenAI gym environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUkTMiZ14scg",
        "outputId": "663e23a8-b3ee-44b6-e54d-2f57c54f9cfa"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fb0efd6eef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOB_-DfM4-vS"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_videos():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  mp4list.sort()\n",
        "  for mp4 in mp4list:\n",
        "    print(f\"\\nSHOWING VIDEO {mp4}\")\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    \n",
        "def wrap_env(env, video_callable=None):\n",
        "  env = Monitor(env, './video', force=True, video_callable=video_callable)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okw8Y8cC5QeS"
      },
      "source": [
        "## Replay Memory\n",
        "Definition of the replay memory, which is a list of finite length storing at each time step (state, action, next_state, reward)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9k6YTJC5PQh"
      },
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
        "\n",
        "    def push(self, state, action, next_state, reward):\n",
        "        self.memory.append( (state, action, next_state, reward) ) # Add the tuple (state, action, next_state, reward) to the queue\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
        "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory) # Return the number of samples currently stored in the memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaZdrL0s6ZjG"
      },
      "source": [
        "## Policies\n",
        "Definition of the policies used for the evaluation of the current state and the generation of associated Q-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taaR_W96nBk"
      },
      "source": [
        "### $\\epsilon$-Greedy Policy\n",
        "Since we work under the assumption that our network will profit from an early exploration policy, with the divergence between real and expected reward shrinking with the number of iterations, then we define\n",
        "$$\\epsilon=e^{-\\alpha{t}}$$\n",
        "$$t:\\:\\:\\:time\\:step$$\n",
        "$$\\alpha:\\:\\:\\:adjustable\\:parameter$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knv33JxA6Y3g"
      },
      "source": [
        "def choose_action_epsilon_greedy(net, state, epsilon):\n",
        "    \n",
        "    if epsilon > 1 or epsilon < 0:\n",
        "        raise Exception('Epsilon value must be between 0 and 1')\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        net_out = net(state)\n",
        "\n",
        "    best_action = int(net_out.argmax())\n",
        "    # Get the number of possible actions\n",
        "    action_space_dim = net_out.shape[-1]\n",
        "\n",
        "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
        "    if random.random() < epsilon:\n",
        "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
        "        action = random.choice(non_optimal_actions)\n",
        "    else:\n",
        "        action = best_action\n",
        "        \n",
        "    return action, net_out.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kApNkIH2AzNr"
      },
      "source": [
        "### Softmax Policy\n",
        "Implementation of the Softmax policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Or1codBHI5"
      },
      "source": [
        "def choose_action_softmax(net, state, temperature):\n",
        "    \n",
        "    if temperature < 0:\n",
        "        raise Exception('The temperature value must be greater than or equal to 0 ')\n",
        "        \n",
        "    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
        "    if temperature == 0:\n",
        "        return choose_action_epsilon_greedy(net, state, 0)\n",
        "    \n",
        "    # Evaluate the network output from the current state\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        net_out = net(state)\n",
        "\n",
        "    # Apply softmax with temp\n",
        "    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
        "    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n",
        "                \n",
        "    # Sample the action using softmax output as mass pdf\n",
        "    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
        "    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
        "    \n",
        "    return action, net_out.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EoKVjpZMq5G"
      },
      "source": [
        "### Exploration Profiles\n",
        "Definition of the lists of probabilities characterizing $\\epsilon$-greedy policy and temperatures characterizing the softmax policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Z1czamMpvX"
      },
      "source": [
        "n_iter=1000\n",
        "ini_temperature=5\n",
        "\n",
        "def explr_pr(alpha, dil):\n",
        "  global n_iter, ini_temperature\n",
        "\n",
        "  explor_prof_greedy=[np.exp(-alpha*i) for i in range(n_iter)]\n",
        "  explor_prof_soft=[ini_temperature*((2**((-ini_temperature*i)/n_iter))**(i*dil/n_iter)) for i in range(n_iter)]\n",
        "  \n",
        "  return explor_prof_greedy, explor_prof_soft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDiIeqWtD0cG"
      },
      "source": [
        "## Network Definition\n",
        "Initialization of the network that generates actions for any given state.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67zFeDrD__m"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_space_dim, action_space_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "                nn.Linear(state_space_dim, 128),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(128, 128),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(128, action_space_dim)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfhRaP2CEYVe"
      },
      "source": [
        "## Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UImYDUQgEa0V",
        "outputId": "f64fd640-3d82-4e6d-c684-6339de79ea5a"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_space_dim = env.observation_space.shape[0]\n",
        "action_space_dim = env.action_space.n\n",
        "\n",
        "print(f\"STATE SPACE SIZE: {state_space_dim}\")\n",
        "print(f\"ACTION SPACE SIZE: {action_space_dim}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STATE SPACE SIZE: 4\n",
            "ACTION SPACE SIZE: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxlfd4-jLrAI"
      },
      "source": [
        "## Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd_7Yqx0LwOt"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2l8eFmJziG"
      },
      "source": [
        "# Random seeds\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# PARAMETERS\n",
        "gamma = 0.97   # LongTerm reward\n",
        "replay_memory_capacity = 10000   # Replay memory capacity\n",
        "lr = 1e-2   # Optimizer learning rate\n",
        "target_net_update_steps = 10   # Number of episodes to wait before updating the target network\n",
        "batch_size = 128   # Number of samples to take from the replay memory for each update\n",
        "bad_state_penalty = 1   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
        "min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training\n",
        "\n",
        "scores=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQnb2TJDKX73"
      },
      "source": [
        "def initialization(replay_memory_capacity, state_space_dim, action_space_dim, lr):\n",
        "\n",
        "  ### Initialize the replay memory\n",
        "  replay_mem = ReplayMemory(replay_memory_capacity)    \n",
        "\n",
        "  ### Initialize the policy network\n",
        "  policy_net = DQN(state_space_dim, action_space_dim)\n",
        "\n",
        "  ### Initialize the target network with the same weights of the policy network\n",
        "  target_net = DQN(state_space_dim, action_space_dim)\n",
        "  target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
        "\n",
        "  ### Initialize the optimizer\n",
        "  optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
        "\n",
        "  return replay_mem, policy_net, target_net, optimizer\n",
        "\n",
        "### Initialize the loss function (Huber loss)\n",
        "loss_fn = nn.SmoothL1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHMLLg8uL0d9"
      },
      "source": [
        "### Update rule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYreYDaOL4Yf"
      },
      "source": [
        "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
        "        \n",
        "    # Sample the data from the replay memory\n",
        "    batch = replay_mem.sample(batch_size)\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # Create tensors for each element of the batch\n",
        "    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n",
        "    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n",
        "    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n",
        "\n",
        "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
        "    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended\n",
        "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
        "\n",
        "    # Compute all the Q values (forward pass)\n",
        "    policy_net.train()\n",
        "    q_values = policy_net(states)\n",
        "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
        "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
        "\n",
        "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
        "    with torch.no_grad():\n",
        "      target_net.eval()\n",
        "      q_values_target = target_net(non_final_next_states)\n",
        "    next_state_max_q_values = torch.zeros(batch_size)\n",
        "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
        "\n",
        "    # Compute the Huber loss\n",
        "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
        "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "746PR4fuO_yd"
      },
      "source": [
        "### Training Loop\n",
        "Initialization of the training loop, which iterates with both the softmax policy and $\\epsilon$-greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25f4c86849684d58ad315672c964fe4a",
            "08ba87bfe6cf4a4591a56d59bec7e290",
            "20a4fae9f57640eba8e8a2993875da44",
            "16b04bb0744749e3931df335f83abda7",
            "f1a0f8d1610a49508d899f901746acc5",
            "0235bb00b4404097987ee9ff7cb49fd3",
            "59b1cbe0a57a4d0aab89aa18821597fd",
            "21bb2cd2d48c48d099cf42ff98c60170"
          ]
        },
        "id": "B_uqznTHPSB7",
        "outputId": "45c50c9a-7e55-4dc8-a0df-2bc6bdacab53"
      },
      "source": [
        "# Initialize the Gym environment\n",
        "env = gym.make('CartPole-v1') \n",
        "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
        "\n",
        "# This is for creating the output video in Colab, not required outside Colab\n",
        "env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes\n",
        "Models=[]\n",
        "# Definition of the values for dilation factors\n",
        "alpha=[0.005, 0.01, 0.02, 0.05, 0.1]\n",
        "dil=[25, 50, 75, 100, 125]\n",
        "pol=\"soft\"\n",
        "\n",
        "for j in range(5):\n",
        "  explor_prof_greedy, explor_prof_soft = explr_pr(alpha[j],dil[j])\n",
        "  if pol==\"soft\":\n",
        "    exploration_profile=explor_prof_soft\n",
        "  elif pol==\"greedy\":\n",
        "    exploration_profile=explor_prof_greedy\n",
        "  replay_mem, policy_net, target_net, optimizer = initialization(replay_memory_capacity, state_space_dim, action_space_dim, lr)\n",
        "  for episode_num, tau in enumerate(tqdm(exploration_profile)):\n",
        "\n",
        "      # Reset the environment and get the initial state\n",
        "      state = env.reset()\n",
        "      # Reset the score. The final score will be the total amount of steps before the pole falls\n",
        "      score = 0\n",
        "      done = False\n",
        "\n",
        "      # Go on until the pole falls off\n",
        "      while not done:\n",
        "\n",
        "        # Choose the action following the policy\n",
        "        if pol==\"soft\":\n",
        "          action, q_values = choose_action_softmax(policy_net, state, temperature=tau)\n",
        "        elif pol==\"greedy\":\n",
        "          action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon=tau)\n",
        "      \n",
        "        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # We apply a (linear) penalty when the cart is far from center\n",
        "        pos_weight = 1\n",
        "        reward = reward - pos_weight * (next_state[0]**4) \n",
        "\n",
        "        # Update the final score (+1 for each step)\n",
        "        score += 1\n",
        "\n",
        "        # Apply penalty for bad state\n",
        "        if done: # if the pole has fallen down \n",
        "            reward += bad_state_penalty\n",
        "            next_state = None\n",
        "\n",
        "        # Update the replay memory\n",
        "        replay_mem.push(state, action, next_state, reward)\n",
        "\n",
        "        # Update the network\n",
        "        if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
        "            update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
        "\n",
        "        # Visually render the environment (disable to speed up the training)\n",
        "        env.render()\n",
        "\n",
        "        # Set the current state for the next iteration\n",
        "        state = next_state\n",
        "\n",
        "      scores.append(score)\n",
        "      # Update the target network every target_net_update_steps episodes\n",
        "      if episode_num % target_net_update_steps == 0:\n",
        "          print('Updating target network...')\n",
        "          target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
        "\n",
        "      # Print the final score\n",
        "      if pol==\"soft\":\n",
        "        print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\")\n",
        "      elif pol==\"greedy\":\n",
        "        print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Epsilon: {tau}\")\n",
        "    \n",
        "      # Early Stop Condition\n",
        "      early_stop=0\n",
        "      if episode_num>50:\n",
        "        early_stop=np.mean(scores[-5:])\n",
        "      if episode_num==999 or early_stop>450:\n",
        "        model={\"policy\":pol, \"episode\":episode_num, \"alpha\":alpha[j], \"dil\":dil[j]}\n",
        "        Models.append(model)\n",
        "        name=\"Policy_param_\"+pol+\"_\"+str(j)+\".pth\"\n",
        "        torch.save(policy_net.state_dict(),name)\n",
        "      if episode_num>50:\n",
        "        if early_stop>450:\n",
        "            break\n",
        "\n",
        "  print(\"---------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"---------------------------------------------------------\")\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25f4c86849684d58ad315672c964fe4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Updating target network...\n",
            "EPISODE: 1 - FINAL SCORE: 12 - Temperature: 5.0\n",
            "EPISODE: 2 - FINAL SCORE: 17 - Temperature: 4.999566801779304\n",
            "EPISODE: 3 - FINAL SCORE: 41 - Temperature: 4.998267432297046\n",
            "EPISODE: 4 - FINAL SCORE: 19 - Temperature: 4.99610256689765\n",
            "EPISODE: 5 - FINAL SCORE: 24 - Temperature: 4.993073330505144\n",
            "EPISODE: 6 - FINAL SCORE: 16 - Temperature: 4.98918129664899\n",
            "EPISODE: 7 - FINAL SCORE: 30 - Temperature: 4.984428486101879\n",
            "EPISODE: 8 - FINAL SCORE: 29 - Temperature: 4.978817365131169\n",
            "EPISODE: 9 - FINAL SCORE: 15 - Temperature: 4.972350843366072\n",
            "EPISODE: 10 - FINAL SCORE: 21 - Temperature: 4.965032271283169\n",
            "Updating target network...\n",
            "EPISODE: 11 - FINAL SCORE: 31 - Temperature: 4.9568654373133105\n",
            "EPISODE: 12 - FINAL SCORE: 59 - Temperature: 4.947854564573375\n",
            "EPISODE: 13 - FINAL SCORE: 29 - Temperature: 4.938004307226863\n",
            "EPISODE: 14 - FINAL SCORE: 26 - Temperature: 4.927319746477684\n",
            "EPISODE: 15 - FINAL SCORE: 18 - Temperature: 4.9158063862019965\n",
            "EPISODE: 16 - FINAL SCORE: 14 - Temperature: 4.903470148223333\n",
            "EPISODE: 17 - FINAL SCORE: 17 - Temperature: 4.890317367236697\n",
            "EPISODE: 18 - FINAL SCORE: 27 - Temperature: 4.876354785387729\n",
            "EPISODE: 19 - FINAL SCORE: 20 - Temperature: 4.861589546513423\n",
            "EPISODE: 20 - FINAL SCORE: 67 - Temperature: 4.84602919005129\n",
            "Updating target network...\n",
            "EPISODE: 21 - FINAL SCORE: 16 - Temperature: 4.829681644624228\n",
            "EPISODE: 22 - FINAL SCORE: 25 - Temperature: 4.812555221308753\n",
            "EPISODE: 23 - FINAL SCORE: 18 - Temperature: 4.794658606594577\n",
            "EPISODE: 24 - FINAL SCORE: 28 - Temperature: 4.776000855043884\n",
            "EPISODE: 25 - FINAL SCORE: 11 - Temperature: 4.756591381658996\n",
            "EPISODE: 26 - FINAL SCORE: 30 - Temperature: 4.736439953967414\n",
            "EPISODE: 27 - FINAL SCORE: 13 - Temperature: 4.715556683833569\n",
            "EPISODE: 28 - FINAL SCORE: 23 - Temperature: 4.693952019006861\n",
            "EPISODE: 29 - FINAL SCORE: 14 - Temperature: 4.671636734415902\n",
            "EPISODE: 30 - FINAL SCORE: 26 - Temperature: 4.648621923219075\n",
            "Updating target network...\n",
            "EPISODE: 31 - FINAL SCORE: 13 - Temperature: 4.624918987621843\n",
            "EPISODE: 32 - FINAL SCORE: 26 - Temperature: 4.600539629471418\n",
            "EPISODE: 33 - FINAL SCORE: 16 - Temperature: 4.5754958406396415\n",
            "EPISODE: 34 - FINAL SCORE: 23 - Temperature: 4.549799893205139\n",
            "EPISODE: 35 - FINAL SCORE: 13 - Temperature: 4.5234643294459715\n",
            "EPISODE: 36 - FINAL SCORE: 16 - Temperature: 4.496501951654205\n",
            "EPISODE: 37 - FINAL SCORE: 17 - Temperature: 4.468925811783935\n",
            "EPISODE: 38 - FINAL SCORE: 40 - Temperature: 4.440749200944491\n",
            "EPISODE: 39 - FINAL SCORE: 19 - Temperature: 4.411985638750601\n",
            "EPISODE: 40 - FINAL SCORE: 22 - Temperature: 4.3826488625414495\n",
            "Updating target network...\n",
            "EPISODE: 41 - FINAL SCORE: 16 - Temperature: 4.352752816480621\n",
            "EPISODE: 42 - FINAL SCORE: 26 - Temperature: 4.322311640548998\n",
            "EPISODE: 43 - FINAL SCORE: 16 - Temperature: 4.291339659442725\n",
            "EPISODE: 44 - FINAL SCORE: 35 - Temperature: 4.259851371388389\n",
            "EPISODE: 45 - FINAL SCORE: 18 - Temperature: 4.227861436887582\n",
            "EPISODE: 46 - FINAL SCORE: 23 - Temperature: 4.195384667403015\n",
            "EPISODE: 47 - FINAL SCORE: 20 - Temperature: 4.162436013998315\n",
            "EPISODE: 48 - FINAL SCORE: 16 - Temperature: 4.129030555943637\n",
            "EPISODE: 49 - FINAL SCORE: 26 - Temperature: 4.095183489299139\n",
            "EPISODE: 50 - FINAL SCORE: 14 - Temperature: 4.06091011548832\n",
            "Updating target network...\n",
            "EPISODE: 51 - FINAL SCORE: 52 - Temperature: 4.026225829873136\n",
            "EPISODE: 52 - FINAL SCORE: 17 - Temperature: 3.9911461103426955\n",
            "EPISODE: 53 - FINAL SCORE: 37 - Temperature: 3.9556865059272495\n",
            "EPISODE: 54 - FINAL SCORE: 14 - Temperature: 3.9198626254490336\n",
            "EPISODE: 55 - FINAL SCORE: 18 - Temperature: 3.883690126221392\n",
            "EPISODE: 56 - FINAL SCORE: 43 - Temperature: 3.8471847028074495\n",
            "EPISODE: 57 - FINAL SCORE: 20 - Temperature: 3.810362075849423\n",
            "EPISODE: 58 - FINAL SCORE: 17 - Temperature: 3.773237980979482\n",
            "EPISODE: 59 - FINAL SCORE: 17 - Temperature: 3.735828157822861\n",
            "EPISODE: 60 - FINAL SCORE: 44 - Temperature: 3.69814833910373\n",
            "Updating target network...\n",
            "EPISODE: 61 - FINAL SCORE: 13 - Temperature: 3.660214239864064\n",
            "EPISODE: 62 - FINAL SCORE: 16 - Temperature: 3.622041546805575\n",
            "EPISODE: 63 - FINAL SCORE: 17 - Temperature: 3.5836459077644722\n",
            "EPISODE: 64 - FINAL SCORE: 20 - Temperature: 3.54504292132857\n",
            "EPISODE: 65 - FINAL SCORE: 44 - Temperature: 3.5062481266060033\n",
            "EPISODE: 66 - FINAL SCORE: 16 - Temperature: 3.467276993154523\n",
            "EPISODE: 67 - FINAL SCORE: 25 - Temperature: 3.428144911080055\n",
            "EPISODE: 68 - FINAL SCORE: 11 - Temperature: 3.388867181312887\n",
            "EPISODE: 69 - FINAL SCORE: 13 - Temperature: 3.349459006069602\n",
            "EPISODE: 70 - FINAL SCORE: 77 - Temperature: 3.3099354795084794\n",
            "Updating target network...\n",
            "EPISODE: 71 - FINAL SCORE: 53 - Temperature: 3.2703115785858277\n",
            "EPISODE: 72 - FINAL SCORE: 51 - Temperature: 3.2306021541203584\n",
            "EPISODE: 73 - FINAL SCORE: 29 - Temperature: 3.1908219220723826\n",
            "EPISODE: 74 - FINAL SCORE: 21 - Temperature: 3.150985455044256\n",
            "EPISODE: 75 - FINAL SCORE: 24 - Temperature: 3.1111071740081946\n",
            "EPISODE: 76 - FINAL SCORE: 20 - Temperature: 3.0712013402671747\n",
            "EPISODE: 77 - FINAL SCORE: 26 - Temperature: 3.0312820476543427\n",
            "EPISODE: 78 - FINAL SCORE: 72 - Temperature: 2.991363214975951\n",
            "EPISODE: 79 - FINAL SCORE: 21 - Temperature: 2.951458578702517\n",
            "EPISODE: 80 - FINAL SCORE: 24 - Temperature: 2.9115816859125183\n",
            "Updating target network...\n",
            "EPISODE: 81 - FINAL SCORE: 21 - Temperature: 2.871745887492587\n",
            "EPISODE: 82 - FINAL SCORE: 37 - Temperature: 2.8319643315978142\n",
            "EPISODE: 83 - FINAL SCORE: 25 - Temperature: 2.7922499573753763\n",
            "EPISODE: 84 - FINAL SCORE: 28 - Temperature: 2.7526154889543935\n",
            "EPISODE: 85 - FINAL SCORE: 17 - Temperature: 2.713073429704518\n",
            "EPISODE: 86 - FINAL SCORE: 42 - Temperature: 2.673636056765401\n",
            "EPISODE: 87 - FINAL SCORE: 22 - Temperature: 2.634315415848868\n",
            "EPISODE: 88 - FINAL SCORE: 19 - Temperature: 2.5951233163152256\n",
            "EPISODE: 89 - FINAL SCORE: 9 - Temperature: 2.5560713265248056\n",
            "EPISODE: 90 - FINAL SCORE: 69 - Temperature: 2.517170769465494\n",
            "Updating target network...\n",
            "EPISODE: 91 - FINAL SCORE: 39 - Temperature: 2.4784327186566553\n",
            "EPISODE: 92 - FINAL SCORE: 15 - Temperature: 2.439867994329506\n",
            "EPISODE: 93 - FINAL SCORE: 12 - Temperature: 2.401487159883692\n",
            "EPISODE: 94 - FINAL SCORE: 19 - Temperature: 2.36330051861945\n",
            "EPISODE: 95 - FINAL SCORE: 16 - Temperature: 2.3253181107444583\n",
            "EPISODE: 96 - FINAL SCORE: 8 - Temperature: 2.2875497106541243\n",
            "EPISODE: 97 - FINAL SCORE: 35 - Temperature: 2.25000482448378\n",
            "EPISODE: 98 - FINAL SCORE: 43 - Temperature: 2.2126926879309337\n",
            "EPISODE: 99 - FINAL SCORE: 20 - Temperature: 2.1756222643454324\n",
            "EPISODE: 100 - FINAL SCORE: 14 - Temperature: 2.138802243085124\n",
            "Updating target network...\n",
            "EPISODE: 101 - FINAL SCORE: 15 - Temperature: 2.1022410381342866\n",
            "EPISODE: 102 - FINAL SCORE: 36 - Temperature: 2.0659467869818706\n",
            "EPISODE: 103 - FINAL SCORE: 17 - Temperature: 2.0299273497563024\n",
            "EPISODE: 104 - FINAL SCORE: 13 - Temperature: 1.994190308613355\n",
            "EPISODE: 105 - FINAL SCORE: 28 - Temperature: 1.9587429673733516\n",
            "EPISODE: 106 - FINAL SCORE: 40 - Temperature: 1.9235923514037248\n",
            "EPISODE: 107 - FINAL SCORE: 40 - Temperature: 1.8887452077427402\n",
            "EPISODE: 108 - FINAL SCORE: 34 - Temperature: 1.8542080054599643\n",
            "EPISODE: 109 - FINAL SCORE: 18 - Temperature: 1.8199869362488577\n",
            "EPISODE: 110 - FINAL SCORE: 12 - Temperature: 1.7860879152466855\n",
            "Updating target network...\n",
            "EPISODE: 111 - FINAL SCORE: 49 - Temperature: 1.7525165820767312\n",
            "EPISODE: 112 - FINAL SCORE: 27 - Temperature: 1.7192783021076516\n",
            "EPISODE: 113 - FINAL SCORE: 15 - Temperature: 1.6863781679246235\n",
            "EPISODE: 114 - FINAL SCORE: 21 - Temperature: 1.6538210010067889\n",
            "EPISODE: 115 - FINAL SCORE: 20 - Temperature: 1.6216113536053616\n",
            "EPISODE: 116 - FINAL SCORE: 29 - Temperature: 1.589753510816617\n",
            "EPISODE: 117 - FINAL SCORE: 16 - Temperature: 1.5582514928438718\n",
            "EPISODE: 118 - FINAL SCORE: 21 - Temperature: 1.5271090574424504\n",
            "EPISODE: 119 - FINAL SCORE: 27 - Temperature: 1.4963297025415119\n",
            "EPISODE: 120 - FINAL SCORE: 30 - Temperature: 1.4659166690365542\n",
            "Updating target network...\n",
            "EPISODE: 121 - FINAL SCORE: 19 - Temperature: 1.4358729437462936\n",
            "EPISODE: 122 - FINAL SCORE: 35 - Temperature: 1.4062012625275755\n",
            "EPISODE: 123 - FINAL SCORE: 37 - Temperature: 1.376904113541892\n",
            "EPISODE: 124 - FINAL SCORE: 17 - Temperature: 1.347983740667048\n",
            "EPISODE: 125 - FINAL SCORE: 18 - Temperature: 1.3194421470474584\n",
            "EPISODE: 126 - FINAL SCORE: 17 - Temperature: 1.2912810987765355\n",
            "EPISODE: 127 - FINAL SCORE: 21 - Temperature: 1.2635021287046162\n",
            "EPISODE: 128 - FINAL SCORE: 20 - Temperature: 1.2361065403658402\n",
            "EPISODE: 129 - FINAL SCORE: 44 - Temperature: 1.2090954120174153\n",
            "EPISODE: 130 - FINAL SCORE: 26 - Temperature: 1.1824696007846853\n",
            "Updating target network...\n",
            "EPISODE: 131 - FINAL SCORE: 20 - Temperature: 1.1562297469054612\n",
            "EPISODE: 132 - FINAL SCORE: 46 - Temperature: 1.1303762780670616\n",
            "EPISODE: 133 - FINAL SCORE: 17 - Temperature: 1.1049094138295892\n",
            "EPISODE: 134 - FINAL SCORE: 13 - Temperature: 1.0798291701289529\n",
            "EPISODE: 135 - FINAL SCORE: 22 - Temperature: 1.0551353638532537\n",
            "EPISODE: 136 - FINAL SCORE: 39 - Temperature: 1.0308276174861501\n",
            "EPISODE: 137 - FINAL SCORE: 26 - Temperature: 1.0069053638109375\n",
            "EPISODE: 138 - FINAL SCORE: 52 - Temperature: 0.983367850669098\n",
            "EPISODE: 139 - FINAL SCORE: 20 - Temperature: 0.9602141457671866\n",
            "EPISODE: 140 - FINAL SCORE: 47 - Temperature: 0.9374431415259944\n",
            "Updating target network...\n",
            "EPISODE: 141 - FINAL SCORE: 72 - Temperature: 0.9150535599660159\n",
            "EPISODE: 142 - FINAL SCORE: 14 - Temperature: 0.8930439576233429\n",
            "EPISODE: 143 - FINAL SCORE: 39 - Temperature: 0.8714127304902265\n",
            "EPISODE: 144 - FINAL SCORE: 9 - Temperature: 0.8501581189746279\n",
            "EPISODE: 145 - FINAL SCORE: 15 - Temperature: 0.8292782128732262\n",
            "EPISODE: 146 - FINAL SCORE: 62 - Temperature: 0.808770956352428\n",
            "EPISODE: 147 - FINAL SCORE: 47 - Temperature: 0.7886341529320944\n",
            "EPISODE: 148 - FINAL SCORE: 14 - Temperature: 0.7688654704667798\n",
            "EPISODE: 149 - FINAL SCORE: 49 - Temperature: 0.7494624461194452\n",
            "EPISODE: 150 - FINAL SCORE: 20 - Temperature: 0.7304224913227203\n",
            "Updating target network...\n",
            "EPISODE: 151 - FINAL SCORE: 17 - Temperature: 0.7117428967229322\n",
            "EPISODE: 152 - FINAL SCORE: 36 - Temperature: 0.6934208371022734\n",
            "EPISODE: 153 - FINAL SCORE: 15 - Temperature: 0.6754533762745946\n",
            "EPISODE: 154 - FINAL SCORE: 17 - Temperature: 0.6578374719504904\n",
            "EPISODE: 155 - FINAL SCORE: 28 - Temperature: 0.6405699805674635\n",
            "EPISODE: 156 - FINAL SCORE: 29 - Temperature: 0.6236476620811239\n",
            "EPISODE: 157 - FINAL SCORE: 51 - Temperature: 0.6070671847135208\n",
            "EPISODE: 158 - FINAL SCORE: 26 - Temperature: 0.5908251296548626\n",
            "EPISODE: 159 - FINAL SCORE: 17 - Temperature: 0.5749179957150307\n",
            "EPISODE: 160 - FINAL SCORE: 41 - Temperature: 0.559342203921457\n",
            "Updating target network...\n",
            "EPISODE: 161 - FINAL SCORE: 19 - Temperature: 0.5440941020600774\n",
            "EPISODE: 162 - FINAL SCORE: 19 - Temperature: 0.529169969156242\n",
            "EPISODE: 163 - FINAL SCORE: 14 - Temperature: 0.5145660198926132\n",
            "EPISODE: 164 - FINAL SCORE: 26 - Temperature: 0.5002784089612371\n",
            "EPISODE: 165 - FINAL SCORE: 37 - Temperature: 0.486303235347137\n",
            "EPISODE: 166 - FINAL SCORE: 57 - Temperature: 0.47263654654091924\n",
            "EPISODE: 167 - FINAL SCORE: 35 - Temperature: 0.45927434267805534\n",
            "EPISODE: 168 - FINAL SCORE: 78 - Temperature: 0.446212580602636\n",
            "EPISODE: 169 - FINAL SCORE: 100 - Temperature: 0.4334471778535579\n",
            "EPISODE: 170 - FINAL SCORE: 65 - Temperature: 0.420974016571256\n",
            "Updating target network...\n",
            "EPISODE: 171 - FINAL SCORE: 59 - Temperature: 0.40878894732322857\n",
            "EPISODE: 172 - FINAL SCORE: 25 - Temperature: 0.3968877928467685\n",
            "EPISODE: 173 - FINAL SCORE: 77 - Temperature: 0.38526635170743956\n",
            "EPISODE: 174 - FINAL SCORE: 50 - Temperature: 0.3739204018719939\n",
            "EPISODE: 175 - FINAL SCORE: 15 - Temperature: 0.362845704194561\n",
            "EPISODE: 176 - FINAL SCORE: 67 - Temperature: 0.35203800581507566\n",
            "EPISODE: 177 - FINAL SCORE: 33 - Temperature: 0.34149304346904746\n",
            "EPISODE: 178 - FINAL SCORE: 17 - Temperature: 0.3312065467079139\n",
            "EPISODE: 179 - FINAL SCORE: 73 - Temperature: 0.3211742410293318\n",
            "EPISODE: 180 - FINAL SCORE: 13 - Temperature: 0.3113918509169144\n",
            "Updating target network...\n",
            "EPISODE: 181 - FINAL SCORE: 43 - Temperature: 0.3018551027890142\n",
            "EPISODE: 182 - FINAL SCORE: 20 - Temperature: 0.292559727856299\n",
            "EPISODE: 183 - FINAL SCORE: 19 - Temperature: 0.28350146488796807\n",
            "EPISODE: 184 - FINAL SCORE: 45 - Temperature: 0.27467606288657676\n",
            "EPISODE: 185 - FINAL SCORE: 31 - Temperature: 0.26607928367154526\n",
            "EPISODE: 186 - FINAL SCORE: 28 - Temperature: 0.2577069043715374\n",
            "EPISODE: 187 - FINAL SCORE: 34 - Temperature: 0.2495547198259909\n",
            "EPISODE: 188 - FINAL SCORE: 60 - Temperature: 0.24161854489618992\n",
            "EPISODE: 189 - FINAL SCORE: 25 - Temperature: 0.23389421668635804\n",
            "EPISODE: 190 - FINAL SCORE: 88 - Temperature: 0.22637759667534854\n",
            "Updating target network...\n",
            "EPISODE: 191 - FINAL SCORE: 62 - Temperature: 0.2190645727595915\n",
            "EPISODE: 192 - FINAL SCORE: 20 - Temperature: 0.21195106120804752\n",
            "EPISODE: 193 - FINAL SCORE: 69 - Temperature: 0.20503300852999395\n",
            "EPISODE: 194 - FINAL SCORE: 32 - Temperature: 0.19830639325655058\n",
            "EPISODE: 195 - FINAL SCORE: 107 - Temperature: 0.19176722763692475\n",
            "EPISODE: 196 - FINAL SCORE: 27 - Temperature: 0.1854115592504214\n",
            "EPISODE: 197 - FINAL SCORE: 119 - Temperature: 0.17923547253533822\n",
            "EPISODE: 198 - FINAL SCORE: 84 - Temperature: 0.17323509023591974\n",
            "EPISODE: 199 - FINAL SCORE: 32 - Temperature: 0.1674065747686061\n",
            "EPISODE: 200 - FINAL SCORE: 203 - Temperature: 0.1617461295088729\n",
            "Updating target network...\n",
            "EPISODE: 201 - FINAL SCORE: 37 - Temperature: 0.15625\n",
            "EPISODE: 202 - FINAL SCORE: 57 - Temperature: 0.1509144750851635\n",
            "EPISODE: 203 - FINAL SCORE: 150 - Temperature: 0.14573588796428474\n",
            "EPISODE: 204 - FINAL SCORE: 84 - Temperature: 0.1407106171771145\n",
            "EPISODE: 205 - FINAL SCORE: 74 - Temperature: 0.13583508751406595\n",
            "EPISODE: 206 - FINAL SCORE: 83 - Temperature: 0.1311057708563443\n",
            "EPISODE: 207 - FINAL SCORE: 78 - Temperature: 0.12651918694695416\n",
            "EPISODE: 208 - FINAL SCORE: 74 - Temperature: 0.12207190409419116\n",
            "EPISODE: 209 - FINAL SCORE: 34 - Temperature: 0.1177605398092485\n",
            "EPISODE: 210 - FINAL SCORE: 64 - Temperature: 0.1135817613795955\n",
            "Updating target network...\n",
            "EPISODE: 211 - FINAL SCORE: 46 - Temperature: 0.10953228637979569\n",
            "EPISODE: 212 - FINAL SCORE: 107 - Temperature: 0.10560888312145661\n",
            "EPISODE: 213 - FINAL SCORE: 53 - Temperature: 0.10180837104400733\n",
            "EPISODE: 214 - FINAL SCORE: 52 - Temperature: 0.09812762104801848\n",
            "EPISODE: 215 - FINAL SCORE: 112 - Temperature: 0.0945635557727789\n",
            "EPISODE: 216 - FINAL SCORE: 50 - Temperature: 0.0911131498198537\n",
            "EPISODE: 217 - FINAL SCORE: 142 - Temperature: 0.08777342992434724\n",
            "EPISODE: 218 - FINAL SCORE: 79 - Temperature: 0.08454147507559648\n",
            "EPISODE: 219 - FINAL SCORE: 77 - Temperature: 0.08141441658901462\n",
            "EPISODE: 220 - FINAL SCORE: 174 - Temperature: 0.07838943813080533\n",
            "Updating target network...\n",
            "EPISODE: 221 - FINAL SCORE: 87 - Temperature: 0.07546377569725356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwiMLv9YPsS3"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oWh3xNzPrjJ"
      },
      "source": [
        "# Initialize the Gym environment\n",
        "env = gym.make('CartPole-v1') \n",
        "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
        "\n",
        "# This is for creating the output video in Colab, not required outside Colab\n",
        "env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
        "mean_scores=[]\n",
        "\n",
        "# Let's try for a total of 10 episodes\n",
        "for j in range(5):\n",
        "  _ ,policy_net, _, _ = initialization(replay_memory_capacity, state_space_dim, action_space_dim, lr)\n",
        "  name=\"Policy_param_\"+pol+\"_\"+str(j)+\".pth\"\n",
        "  policy_net.load_state_dict(torch.load(name))\n",
        "  score_m=[]\n",
        "  for num_episode in range(10): \n",
        "      # Reset the environment and get the initial state\n",
        "      state = env.reset()\n",
        "      # Reset the score. The final score will be the total amount of steps before the pole falls\n",
        "      score = 0\n",
        "      done = False\n",
        "      # Go on until the pole falls off or the score reach 490\n",
        "      while not done:\n",
        "        # Choose the best action (temperature 0)\n",
        "        action, q_values = choose_action_softmax(policy_net, state, temperature=0)\n",
        "        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # Visually render the environment\n",
        "        env.render()\n",
        "        # Update the final score (+1 for each step)\n",
        "        score += reward \n",
        "        # Set the current state for the next iteration\n",
        "        state = next_state\n",
        "        # Check if the episode ended (the pole fell down)\n",
        "      # Print the final score\n",
        "      score_m.append(score)\n",
        "      print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\")\n",
        "      if num_episode==9:\n",
        "        mean_scores.append(np.mean(score_m))\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"----------------------------------------------------------\")\n",
        "env.close()\n",
        "\n",
        "for j in range(5):\n",
        "  print(Models[j])\n",
        "  print(\"FINAL MEAN SCORE: \"+str(mean_scores[j]))\n",
        "  explor_prof_greedy, explor_prof_soft = explr_pr(alpha[j],dil[j])\n",
        "\n",
        "  plt.figure(figsize=(12,8))\n",
        "  if pol==\"greedy\":\n",
        "    plt.plot(explor_prof_greedy)\n",
        "  elif pol==\"soft\":\n",
        "    plt.plot(explor_prof_soft)\n",
        "  plt.grid()\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Exploration Profiles\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"----------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"----------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLt_NRpaP0qR"
      },
      "source": [
        "# Not required outside Colab\n",
        "show_videos()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}